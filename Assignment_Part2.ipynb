{"cells":[{"cell_type":"markdown","id":"5dc37583-4eed-478a-960b-76691103c2bd","metadata":{"id":"5dc37583-4eed-478a-960b-76691103c2bd"},"source":["Enter your username (used for marking):"]},{"cell_type":"code","execution_count":null,"id":"cb4274d7-9249-4bb2-859b-82b87567f904","metadata":{"id":"cb4274d7-9249-4bb2-859b-82b87567f904"},"outputs":[],"source":["username = 'myusername'"]},{"cell_type":"markdown","id":"f0aeadec-db99-4452-990e-d0be11ecb37e","metadata":{"id":"f0aeadec-db99-4452-990e-d0be11ecb37e"},"source":["# COM4509/6509 Coursework Part 2\n","\n","Hello,\n","This is the *second* of the two parts. Each part accounts for 50\\% of the overall coursework mark and this part has a total of 50 marks available.\n","Attempt as much of this as you can, each of the questions are self-contained and contain some easier and harder bits so even if you can't complete Q1 straight away then you may still be able to progress with the other questions.\n","\n","### Overview\n","This part of the assignment will cover:\n","- Logistic regression and PCA: 13 Marks, lecture 6 and 8. \n","- Neural networks: 16 Marks, lecture 7 and 8.\n","- Auto-encoders: 16 Marks, lecture 8 and 9.\n","\n","### What to submit\n","\n","- You need to submit **two jupyter notebooks** (not zipped together) and a **pdf** copy of part 2, named:\n","\n","```\n","assignment_part1_[username].ipynb\n","assignment_part2_[username].ipynb\n","assignment_part2_[username].pdf\n","```\n","\n","replacing `[username]` with your username, e.g. `abc18de`.\n","- **Please execute the cells before your submission**. The **pdf** copy will be used as a backup in case the data gets corrupted and since we cannot run all the notebooks during marking. The best way to get a pdf is using Jupyter Notebook locally but if you are using Google Colab and are unable to download it to use Jupyter then you can use the Google Colab *file $\\rightarrow$ print* to get a pdf copy.\n","- **Please do not upload** the data files used in this Notebook. We just want the two python notebooks *and the pdf*.\n","\n","### Assessment Criteria \n","\n","- The marks are indicated for each part: You'll get marks for correct code that does what is asked and gets the right answer. **These contribute 45**. You should make sure any figures are plotted properly with axis labels and figure legends.\n","- There are also **5 marks for \"Code quality\"** (includes both readability and efficiency).\n","\n","### Late submissions\n","\n","We follow the department's guidelines about late submissions, Undergraduate [handbook link](https://sites.google.com/sheffield.ac.uk/comughandbook/your-study/assessment/late-submission). PGT [handbook link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/home/your-study/assessment/late-submission).\n","\n","### Use of unfair means\n","\n","This is an individual assignment, while you may discuss this with your classmates, please make sure you submit your own code. You are allowed to use code from the labs as a basis of your submission.\n","\n","\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\" (from the students Handbook).\n","\n","\n","\n","### Reproducibility and readibility\n","Whenever there is randomness in the computation, you MUST set a random seed for reproducibility. Use your UCard number XXXXXXXXX (or the digits in your registration number if you do not have one) as the random seed throughout this assignment. You can set the seeds using torch.manual_seed(XXXXX) and np.random.seed(XXXXX).\n","Answers for each question should be clearly indicated in your notebook. While code segments are indicated for answers, you may use more cells as necessary.  All code should be clearly documented and explained.\n","Note: You will make several design choices (e.g. hyperparameters) in this assignment. There are no “standard answers”. You are encouraged to explore several design choices to settle down with good/best ones, if time permits.\n","\n"]},{"cell_type":"markdown","id":"db3221a8-3928-4f5d-a0c0-300545d4ee63","metadata":{"tags":[],"id":"db3221a8-3928-4f5d-a0c0-300545d4ee63"},"source":["## Question 1: Logistic regression and PCA [13 marks]\n","\n","MedMNIST is a collection of healthcase based datasets that are pre-processed to match to format of the original MNIST dataset. In this questions, you will perform logistic regression and dimension reduction using PCA on the **PneumoniaMNIST** dataset from the MedMNIST. The task for this dataset is to detect whether a chest X-ray shows signs of Pneumonia or not and is therefore a binary classification task.\n"]},{"cell_type":"markdown","id":"18811903-d606-4a34-b7fa-6a25982f0ba6","metadata":{"id":"18811903-d606-4a34-b7fa-6a25982f0ba6"},"source":["### 1.1: Data download [1 mark]\n","\n","The code cell belows provides the code to download the dataset as a compressed numpy file directly from the [MedMNIST website](https://doi.org/10.5281/zenodo.6496656\n","). If you prefer, you can follow the instructions at https://github.com/MedMNIST/MedMNIST to download and load the data.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a84c1070-52fc-45b5-9b83-bfa425ab8472","metadata":{"id":"a84c1070-52fc-45b5-9b83-bfa425ab8472","outputId":"9fd8fc1b-6cd4-4b2d-83c8-4012e6fa2e3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_images (4708, 28, 28) uint8\n","val_images (524, 28, 28) uint8\n","test_images (624, 28, 28) uint8\n","train_labels (4708, 1) uint8\n","val_labels (524, 1) uint8\n","test_labels (624, 1) uint8\n"]}],"source":["import numpy as np\n","import urllib.request\n","import os\n","\n","# Download the dataset to the local folder\n","urllib.request.urlretrieve('https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1', 'pneumoniamnist.npz')\n","\n","# Load the compressed numpy array file\n","dataset = np.load('./pneumoniamnist.npz')\n","\n","# The loaded dataset contains each array internally\n","for key in dataset.keys():\n","    print(key, dataset[key].shape, dataset[key].dtype)"]},{"cell_type":"markdown","id":"43e5ce3f-041e-4a7e-ba10-da71c9798584","metadata":{"id":"43e5ce3f-041e-4a7e-ba10-da71c9798584"},"source":["**1.1a** After downloading the data, merge the validation set into the training set and reshape the images so that each is a 1D array. Then scale the pixel values so they are in the range [0,1]."]},{"cell_type":"code","execution_count":null,"id":"5135af20-67ef-44c9-b5c5-c2eef1cb2039","metadata":{"id":"5135af20-67ef-44c9-b5c5-c2eef1cb2039"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"dbafb103-1d93-4019-98c2-d55af545782a","metadata":{"id":"dbafb103-1d93-4019-98c2-d55af545782a"},"source":["### 1.2: Dimensional reduction and training [6 marks]\n","\n","**1.2a** Using the Scikit-learn PCA class, transform the training and test data into **at least seven** different sets of reduced dimensions, i.e create 7 alternate datsets with ($k_1, k_2, ..., k_7$) number of features. **Briefly explain** your choice reduced features. Keep a copy of the unreduced data so that in total you have **eight** datasets.\n","\n","\n","You should fit the tranformation based on the training data and use that to transform the test data. You can find details of the PCA transformation class [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."]},{"cell_type":"code","execution_count":null,"id":"9935824e-9a6c-4f9d-9643-f389bc4c7439","metadata":{"id":"9935824e-9a6c-4f9d-9643-f389bc4c7439"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"06059816-4c87-4e7e-ad0d-c7c901e3c502","metadata":{"id":"06059816-4c87-4e7e-ad0d-c7c901e3c502"},"source":["\n","**1.2b** Train **eight** logistic regression classifiers (LRC): one on the original features (unreduced), and seven on PCA features with seven different dimensions in 1.2a, i.e., LRC on $k_1$ PCA features; LRC on $k_2$ PCA features; ..., LRC on $k_7$ PCA features and LRC on the unreduced data. You will need to decide on any options for the logistic regression fitting and **explain** which choices you make.\n","You can use the Scikit Learn Logistic Regression classifier, further information is given [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n","\n"]},{"cell_type":"code","execution_count":null,"id":"40de0dd1-ca0c-469d-95a8-6d404c740088","metadata":{"id":"40de0dd1-ca0c-469d-95a8-6d404c740088"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"f71a9359-2f8e-41d4-a890-0a37e339c1d0","metadata":{"id":"f71a9359-2f8e-41d4-a890-0a37e339c1d0"},"source":["### 1.3: Model evaluation [6 marks]\n","\n","**1.3b** For each of the trained classifiers in 1.2b, calculate the classification accuracy on the training data and the test data. Extract the total explained variance by summing the `PCA.explained_variance_ratio_` for each of your PCA transformations. **Plot** the training accuracy and test accuracy against the total explained variance at each $k_n$. You should include the results for the case trained on the original features, which corresponds to a total explained variance of 1."]},{"cell_type":"code","execution_count":null,"id":"816f4b69-7b4f-4ed8-961e-0a8c0072fdd5","metadata":{"id":"816f4b69-7b4f-4ed8-961e-0a8c0072fdd5"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"046c1283-b898-42ce-b30c-1853cf501082","metadata":{"id":"046c1283-b898-42ce-b30c-1853cf501082"},"source":["**1.3b** Describe at least **two** relevant observations from the evaluation results above."]},{"cell_type":"code","execution_count":null,"id":"2cb3e110-c887-465b-91cf-6a24bbbbf378","metadata":{"id":"2cb3e110-c887-465b-91cf-6a24bbbbf378"},"outputs":[],"source":["# Write your answer here."]},{"cell_type":"markdown","id":"94d478e2-a780-405a-bc7f-d09ac1b83978","metadata":{"tags":[],"id":"94d478e2-a780-405a-bc7f-d09ac1b83978"},"source":["## Question 2: Convolutional neural networks for image recognition [16 marks]\n","\n","Fashion-MNIST is a dataset of Zalando's article images. It consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0=T-shirt/top; 1=Trouser; 2=Pullover; 3=Dress; 4=Coat; 5=Sandal; 6=Shirt; 7=Sneaker; 8=Bag; 9=Ankle boot.\n","\n","It is available online at https://github.com/zalandoresearch/fashion-mnist but here we will use the version built into PyTorch as part of the TorchVision library [see here for  documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST).\n","\n","In this question, you should PyTorch to train various forms of neural network models to classify these images. You can refer to Lab 7 on how to define and train neural networks with PyTorch. "]},{"cell_type":"markdown","id":"a60f2a44-b5d2-4bd6-9464-3cec57d1fd03","metadata":{"tags":[],"id":"a60f2a44-b5d2-4bd6-9464-3cec57d1fd03"},"source":["### 2.1: Data download and inspection [3 marks]\n","\n","**2.1a** Use the PyTorch Torchvision API to load both the train and test parts of the Fashion-MNIST dataset. You can use the code used in Lab 7 to load the CIFAR10 as a basis for this."]},{"cell_type":"code","execution_count":null,"id":"7037c68e-a830-4971-b58d-62ab0b227e2c","metadata":{"id":"7037c68e-a830-4971-b58d-62ab0b227e2c"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"7f792535-989e-431b-a803-cbcadfcfb08d","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"7f792535-989e-431b-a803-cbcadfcfb08d"},"source":["**2.1b** Use the `torch.utils.data.random_split` function to split the 60,000 training set into 2 subsets: the first part will be used for training, the second part will be used for validation. You must choose a sensible split of this into the training and validation sets. Create a DataLoader for each of the train, validation, and test splits."]},{"cell_type":"code","execution_count":null,"id":"95752d55-c05d-452b-82b5-bd6a7e327222","metadata":{"id":"95752d55-c05d-452b-82b5-bd6a7e327222"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"0a1bd033-2daa-4078-a5e3-e8fdb277ff43","metadata":{"id":"0a1bd033-2daa-4078-a5e3-e8fdb277ff43"},"source":["**2.1c** Display 2 example images from each of the classes (20 images in total)."]},{"cell_type":"code","execution_count":null,"id":"4c08b42f-8962-43d7-ab55-00a06222b121","metadata":{"id":"4c08b42f-8962-43d7-ab55-00a06222b121"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"04681c0a-8879-4e21-9f76-8e7465f61e51","metadata":{"id":"04681c0a-8879-4e21-9f76-8e7465f61e51"},"source":["### 2.2: Network training [8 marks]\n","\n","In this section you will train a set of neural network models to classify the Fashion-MNIST data set. Only the number of convolutional (Conv) layers and the number of fully connected (FC) layers will be specified below. You are free to design other aspects of the network. For example, you can use other types of operation (e.g. padding), layers (e.g. pooling, or preprocessing (e.g. augmentation), and you choose the number of units/neurons in each layer. Likewise, you may choose the number of epochs and many other settings according to your accessible computational power. You should choose sensible values for the batch size and learning rate. If you wish, you may use alternate optimisers, such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n","\n","When training each model you should keep track of the following values:\n","1. Training accuracy\n","2. Validation accuracy\n","3. Test accuracy\n","\n","Remember the accuracy is the number of correct classifications out of that portion of the dataset."]},{"cell_type":"markdown","id":"783d669f-7693-428c-a63a-7c9d4706562e","metadata":{"id":"783d669f-7693-428c-a63a-7c9d4706562e"},"source":["**2.2a** Train a neural network composed of **2 fully connected layers** with an activation function of your choice. Train the model on the training set, use the validation set to choose the best design among **at least three different** choices, and test the chosen model on the test set.\n","\n","Remember that your dataloader will give you a 2D image. The CNNs can process these but your fully connected (`nn.Linear`) layers are expecting each sample to be a vector.\n"]},{"cell_type":"code","execution_count":null,"id":"ab15bb0d-7327-480f-9f1a-c46f142c6cc0","metadata":{"id":"ab15bb0d-7327-480f-9f1a-c46f142c6cc0"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"4565c0f1-5768-4b2e-ad8f-3d29e88a3061","metadata":{"id":"4565c0f1-5768-4b2e-ad8f-3d29e88a3061"},"source":["**2.2b** Define and train using a neural network composed of **2 convolutional layers and 2 fully connected layers**. Train the model on the training set, use the validation set to choose the best design among **at least three different** choices, and test the chosen model on the test set."]},{"cell_type":"code","execution_count":null,"id":"22da30e7-36ba-445d-b671-41d1014b2a28","metadata":{"id":"22da30e7-36ba-445d-b671-41d1014b2a28"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"b8a3c643-dc01-42e0-ad87-9e8a323927c2","metadata":{"id":"b8a3c643-dc01-42e0-ad87-9e8a323927c2"},"source":["**2.2c** Train a neural network composed of **3 convolutional layers and 3 fully connected layers**. Train the model on the training set, use the validation set to choose the best design among **at least three different** choices, and test the chosen model on the test set."]},{"cell_type":"code","execution_count":null,"id":"63d6f794-885b-4da5-9ae9-0e2a1e4a2d93","metadata":{"id":"63d6f794-885b-4da5-9ae9-0e2a1e4a2d93"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"fad0e5c4-7b47-41a9-88cb-c54e75f3162b","metadata":{"tags":[],"id":"fad0e5c4-7b47-41a9-88cb-c54e75f3162b"},"source":["### 2.3: Comparison of model performance [5 marks]\n","\n","**2.3a** In separate **plots**, show the training accuracy, validation accuracy and test accuracy for each of these models.\n"]},{"cell_type":"code","execution_count":null,"id":"8ead459e-81c6-4821-b93b-1e55177462ac","metadata":{"id":"8ead459e-81c6-4821-b93b-1e55177462ac"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"7619fcd3-6cb5-43a6-b8fe-b006c2cca18d","metadata":{"id":"7619fcd3-6cb5-43a6-b8fe-b006c2cca18d"},"source":["**2.3b** Describe at least **two** observations of the data plotted in this section."]},{"cell_type":"code","execution_count":null,"id":"6778cf47-c2b8-4a38-b964-95b5f684e131","metadata":{"id":"6778cf47-c2b8-4a38-b964-95b5f684e131"},"outputs":[],"source":["# Write your answer here"]},{"cell_type":"markdown","id":"ee1e232e-2988-44e0-a8e4-38c3a729c6a7","metadata":{"tags":[],"id":"ee1e232e-2988-44e0-a8e4-38c3a729c6a7"},"source":["## 3. Denoising Autoencoder [16 marks]\n","\n","### The CIFAR-10 dataset\n","In this assignment, we will work on the CIFAR-10 dataset collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton from the University of Toronto. This dataset consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. Each sample is a 3-channel colour images of 32x32 pixels in size. There are 50,000 training images and 10,000 test images."]},{"cell_type":"markdown","id":"4db26803-99bc-4f6e-a5b5-dae78684d0b2","metadata":{"id":"4db26803-99bc-4f6e-a5b5-dae78684d0b2"},"source":["### 3.1: Data loading and manipulation [3 marks]"]},{"cell_type":"markdown","id":"49f74dda-5c06-43cd-a25c-57599b8e2071","metadata":{"id":"49f74dda-5c06-43cd-a25c-57599b8e2071"},"source":["**3.1a** Download both the training and test data of the CIFAR-10 dataset, e.g., by following the pytorch CIFAR10 tutorial. You can also download via other ways if you prefer."]},{"cell_type":"code","execution_count":null,"id":"46e52e7d-6320-44f2-a5d9-fe991fbf8433","metadata":{"id":"46e52e7d-6320-44f2-a5d9-fe991fbf8433"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"51f1b2cf-eca0-4bc7-9e48-47f92b194ae9","metadata":{"id":"51f1b2cf-eca0-4bc7-9e48-47f92b194ae9"},"source":["**3.1b** Add random noise to all training and test data to generate noisy dataset, e.g., by torch.randn(), with a scaling factor scale, e.g., original image + scale * torch.randn(), and normalise/standardise the pixel values to the original range, e.g., using np.clip(). You may choose any scale value between 0.2 and 0.5.\n","\n","A random transformation can be applied using a `Lambda` [transform](https://pytorch.org/vision/stable/transforms.html) when composing the load data transform, which looks a little like this:  \n","`transforms.Lambda(lambda x: x + ..... )`\n","\n","Note: Before generating the random noise, you MUST set the random seed to your UCard number XXXXXXXXX for reproducibility, e.g., using torch.manual_seed(). This seed needs to be used for all remaining code if there is randomness, for reproducibility.\n","\n","You may want to create separate dataloaders for the noisy and clear images but make sure they are not shuffling the data so that correct pair of images are being given as input and desired output."]},{"cell_type":"code","execution_count":null,"id":"ba1fb017-012f-4edf-aee2-def1561af187","metadata":{"id":"ba1fb017-012f-4edf-aee2-def1561af187"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"7182b198-9684-4ecf-95e6-8a09a257adfd","metadata":{"id":"7182b198-9684-4ecf-95e6-8a09a257adfd"},"source":["**3.1c** Show 20 pairs of original and noisy images."]},{"cell_type":"code","execution_count":null,"id":"c2dc6e68-99a9-4901-beb1-750f32505a0c","metadata":{"id":"c2dc6e68-99a9-4901-beb1-750f32505a0c"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"5a126f11-ad6c-4973-986b-f8f44bf4b764","metadata":{"tags":[],"id":"5a126f11-ad6c-4973-986b-f8f44bf4b764"},"source":["\n","### 3.2 Applying a Denoising Autoencoder to the modified CIFAR10 [10 marks]\n","\n","This question uses both the original and noisy CIFAR-10 datasets (all 10 classes).\n","Read about denoising autoencoders at [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder) and this [short introduction](https://towardsdatascience.com/denoising-autoencoders-explained-dbb82467fc2) or any other sources you like."]},{"cell_type":"markdown","id":"9bb043cb-832a-4fda-9245-87bdb84ef610","metadata":{"id":"9bb043cb-832a-4fda-9245-87bdb84ef610"},"source":["**3.2a** Modify the autoencoder architecture in Lab 7 so that it takes colour images as input (i.e., 3 input channels)."]},{"cell_type":"code","execution_count":null,"id":"50b9004b-048b-4057-b707-34d7c29b524e","metadata":{"id":"50b9004b-048b-4057-b707-34d7c29b524e"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"3238f9ba-164e-4801-9811-5db6a064717c","metadata":{"id":"3238f9ba-164e-4801-9811-5db6a064717c"},"source":["**3.2b** Training: feed the noisy training images as input to the autoencoder defined above; use a loss function that computes the reconstruction error between the output of the autoencoder and the respective original images.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a70f5554-a6ca-45e2-9eb9-09d94b95be8e","metadata":{"id":"a70f5554-a6ca-45e2-9eb9-09d94b95be8e"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"d516f3f6-186e-48a1-a538-24b85c7282ea","metadata":{"id":"d516f3f6-186e-48a1-a538-24b85c7282ea"},"source":["**3.2c** Testing: evaluate the autoencoder trained in 3.2b on the test datasets (feed noisy images in and compute reconstruction errors on original clean images. Find the worst denoised 30 images (those with the largest reconstruction errors) in the test set and show them in pairs with the original images (60 images to show in total)."]},{"cell_type":"code","execution_count":null,"id":"f7e40e81-35c8-48dc-822d-e0732c2cc17c","metadata":{"id":"f7e40e81-35c8-48dc-822d-e0732c2cc17c"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"a1a3ee24-c5bf-424e-b7e6-b9d9f92c49e1","metadata":{"id":"a1a3ee24-c5bf-424e-b7e6-b9d9f92c49e1"},"source":["**3.2d** Choose at least **two** hyperparameters (e.g learning rate) to vary. Study at least **three** different choices for each hyperparameter. When varying one hyperparameter, all the other hyperparameters can be fixed. **Plot** the reconstruction error with respect to each of these hyper-parameters."]},{"cell_type":"code","execution_count":null,"id":"1594226b-de91-4554-81f8-d5038bd7256e","metadata":{"id":"1594226b-de91-4554-81f8-d5038bd7256e"},"outputs":[],"source":["# Write your code here."]},{"cell_type":"markdown","id":"a81ae7d1-0da2-465e-9fde-a18be1b4da7d","metadata":{"id":"a81ae7d1-0da2-465e-9fde-a18be1b4da7d"},"source":["### 3.3 Discussion of results [3 marks]\n","**3.3a** Describe at least **two** interesting relevant observations from the evaluation results above."]},{"cell_type":"code","execution_count":null,"id":"51b926a0-404a-426a-adb6-5951fdeab1bd","metadata":{"id":"51b926a0-404a-426a-adb6-5951fdeab1bd"},"outputs":[],"source":["# Write your answer here."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}